{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924d233a",
   "metadata": {},
   "source": [
    "# WhisperPlus: Advancing Speech2Text and Text2Speech Processing üöÄ\n",
    "\n",
    "This Jupyter Notebook demonstrates the capabilities of the WhisperPlus library, an advanced tool for speech-to-text and text-to-speech processing. Below, we have organized different functionalities of WhisperPlus into separate sections, each accompanied by explanatory comments to assist with understanding and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ef5d3",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Installation\n",
    "\n",
    "Before we start, you need to install the WhisperPlus package. Run the following command to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U whisperplus\n",
    "\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b2e40",
   "metadata": {},
   "source": [
    "### üéµ Youtube URL to Audio\n",
    "\n",
    "This section demonstrates how to convert a YouTube video to audio and transcribe it using WhisperPlus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus import SpeechToTextPipeline, download_and_convert_to_mp3\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=di3rHkEZuUw\"\n",
    "audio_path = download_and_convert_to_mp3(url)\n",
    "pipeline = SpeechToTextPipeline(model_id=\"openai/whisper-large-v3\")\n",
    "transcript = pipeline(audio_path, \"openai/whisper-large-v3\", \"english\")\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99131f",
   "metadata": {},
   "source": [
    "### üì∞ Summarization\n",
    "\n",
    "Here, we showcase how to summarize text using the TextSummarizationPipeline in WhisperPlus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus import TextSummarizationPipeline\n",
    "\n",
    "summarizer = TextSummarizationPipeline(model_id=\"facebook/bart-large-cnn\")\n",
    "summary = summarizer.summarize(transcript)\n",
    "print(summary[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae760cb",
   "metadata": {},
   "source": [
    "### üóûÔ∏è Long Text Support Summarization\n",
    "\n",
    "This part shows how to summarize longer texts using the LongTextSupportSummarizationPipeline, which is particularly useful for handling extensive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus import LongTextSummarizationPipeline\n",
    "\n",
    "summarizer = LongTextSummarizationPipeline(model_id=\"facebook/bart-large-cnn\")\n",
    "summary_text = summarizer.summarize(transcript)\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ed60b",
   "metadata": {},
   "source": [
    "### üí¨ Speaker Diarization\n",
    "\n",
    "In this section, we demonstrate the use of Speaker Diarization. This feature helps in distinguishing between different speakers in an audio clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus import (\n",
    "    ASRDiarizationPipeline,\n",
    "    download_and_convert_to_mp3,\n",
    "    format_speech_to_dialogue,\n",
    ")\n",
    "\n",
    "audio_path = download_and_convert_to_mp3(\"https://www.youtube.com/watch?v=mRB14sFHw2E\")\n",
    "\n",
    "device = \"cuda\"  # cpu or mps\n",
    "pipeline = ASRDiarizationPipeline.from_pretrained(\n",
    "    asr_model=\"openai/whisper-large-v3\",\n",
    "    diarizer_model=\"pyannote/speaker-diarization\",\n",
    "    use_auth_token=False,\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "output_text = pipeline(audio_path, num_speakers=2, min_speaker=1, max_speaker=2)\n",
    "dialogue = format_speech_to_dialogue(output_text)\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56941291",
   "metadata": {},
   "source": [
    "### ‚≠ê RAG - Chat with Video (LanceDB)\n",
    "\n",
    "This part covers the 'Chat with Video' feature using LanceDB. It demonstrates how to interact with a video transcript using a chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c108aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus import ChatWithVideo\n",
    "\n",
    "chat = ChatWithVideo(\n",
    "    input_file=\"trascript.txt\",\n",
    "    llm_model_name=\"TheBloke/Mistral-7B-v0.1-GGUF\",\n",
    "    llm_model_file=\"mistral-7b-v0.1.Q4_K_M.gguf\",\n",
    "    llm_model_type=\"mistral\",\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")\n",
    "\n",
    "query = \"what is this video about ?\"\n",
    "response = chat.run_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c4696",
   "metadata": {},
   "source": [
    "### üå† RAG - Chat with Video (AutoLLM)\n",
    "\n",
    "This section demonstrates the 'Chat with Video' feature using AutoLLM. It enables querying a video's content through a chat interface, utilizing advanced language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines.autollm_chatbot import AutoLLMChatWithVideo\n",
    "\n",
    "# service_context_params\n",
    "system_prompt = \"\"\"\n",
    "You are an friendly ai assistant that help users find the most relevant and accurate answers\n",
    "to their questions based on the documents you have access to.\n",
    "When answering the questions, mostly rely on the info in documents.\n",
    "\"\"\"\n",
    "query_wrapper_prompt = \"\"\"\n",
    "The document information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Using the document information and mostly relying on it,\n",
    "answer the query.\n",
    "Query: {query_str}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "chat = AutoLLMChatWithVideo(\n",
    "    input_file=\"audio.mp3\",\n",
    "    openai_key=\"YOUR_OPENAI_KEY\",\n",
    "    huggingface_key=\"YOUR_HUGGINGFACE_KEY\",\n",
    "    llm_model=\"gpt-3.5-turbo\",\n",
    "    llm_max_tokens=\"256\",\n",
    "    llm_temperature=\"0.1\",\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    embed_model=\"huggingface/BAAI/bge-large-zh\",\n",
    ")\n",
    "\n",
    "query = \"what is this video about ?\"\n",
    "response = chat.run_query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ed48e",
   "metadata": {},
   "source": [
    "### üéôÔ∏è Speech to Text\n",
    "\n",
    "Finally, this section covers converting text to speech using WhisperPlus, demonstrating how to generate spoken audio from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50801820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus import TextToSpeechPipeline\n",
    "\n",
    "tts = TextToSpeechPipeline(model_id=\"suno/bark\")\n",
    "audio = tts(text=\"Hello World\", voice_preset=\"v2/en_speaker_6\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
