{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924d233a",
   "metadata": {},
   "source": [
    "# WhisperPlus: Advancing Speech2Text and Text2Speech Processing 🚀\n",
    "\n",
    "This Jupyter Notebook demonstrates the capabilities of the WhisperPlus library, an advanced tool for speech-to-text and text-to-speech processing. Below, we have organized different functionalities of WhisperPlus into separate sections, each accompanied by explanatory comments to assist with understanding and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ef5d3",
   "metadata": {},
   "source": [
    "## 🛠️ Installation\n",
    "\n",
    "Before we start, you need to install the WhisperPlus package. Run the following command to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U whisperplus\n",
    "\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install whisperplus git+https://github.com/huggingface/transformers\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b2e40",
   "metadata": {},
   "source": [
    "### 🎵 Youtube URL to Audio\n",
    "\n",
    "This section demonstrates how to convert a YouTube video to audio and transcribe it using WhisperPlus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus import SpeechToTextPipeline, download_youtube_to_mp3\n",
    "from transformers import BitsAndBytesConfig, HqqConfig\n",
    "import torch\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=di3rHkEZuUw\"\n",
    "audio_path = download_youtube_to_mp3(url, output_dir=\"downloads\", filename=\"test\")\n",
    "\n",
    "hqq_config = HqqConfig(\n",
    "    nbits=4,\n",
    "    group_size=64,\n",
    "    quant_zero=False,\n",
    "    quant_scale=False,\n",
    "    axis=0,\n",
    "    offload_meta=False,\n",
    ")  # axis=0 is used by default\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "pipeline = SpeechToTextPipeline(\n",
    "    model_id=\"distil-whisper/distil-large-v3\",\n",
    "    quant_config=hqq_config,\n",
    "    flash_attention_2=True,\n",
    ")\n",
    "\n",
    "transcript = pipeline(\n",
    "    audio_path=audio_path,\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=5,\n",
    "    max_new_tokens=128,\n",
    "    batch_size=100,\n",
    "    language=\"english\",\n",
    "    return_timestamps=False,\n",
    ")\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6282f7",
   "metadata": {},
   "source": [
    "### 🍎 Apple MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b42087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines import mlx_whisper\n",
    "from whisperplus import download_youtube_to_mp3\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=1__CAdTJ5JU\"\n",
    "audio_path = download_youtube_to_mp3(url)\n",
    "\n",
    "text = mlx_whisper.transcribe(\n",
    "    audio_path, path_or_hf_repo=\"mlx-community/whisper-large-v3-mlx\"\n",
    ")[\"text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca528ba7",
   "metadata": {},
   "source": [
    "### 🍏 Lightning Mlx Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines.lightning_whisper_mlx import LightningWhisperMLX\n",
    "from whisperplus import download_youtube_to_mp3\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=1__CAdTJ5JU\"\n",
    "audio_path = download_youtube_to_mp3(url)\n",
    "\n",
    "whisper = LightningWhisperMLX(model=\"distil-large-v3\", batch_size=12, quant=None)\n",
    "output = whisper.transcribe(audio_path=audio_path)[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99131f",
   "metadata": {},
   "source": [
    "### 📰 Summarization\n",
    "\n",
    "Here, we showcase how to summarize text using the TextSummarizationPipeline in WhisperPlus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines.summarization import TextSummarizationPipeline\n",
    "\n",
    "summarizer = TextSummarizationPipeline(model_id=\"facebook/bart-large-cnn\")\n",
    "summary = summarizer.summarize(transcript)\n",
    "print(summary[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae760cb",
   "metadata": {},
   "source": [
    "### 🗞️ Long Text Support Summarization\n",
    "\n",
    "This part shows how to summarize longer texts using the LongTextSupportSummarizationPipeline, which is particularly useful for handling extensive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines.long_text_summarization import LongTextSummarizationPipeline\n",
    "\n",
    "summarizer = LongTextSummarizationPipeline(model_id=\"facebook/bart-large-cnn\")\n",
    "summary_text = summarizer.summarize(transcript)\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ed60b",
   "metadata": {},
   "source": [
    "### 💬 Speaker Diarization\n",
    "\n",
    "You must confirm the licensing permissions of these two models.\n",
    "\n",
    "- https://huggingface.co/pyannote/speaker-diarization-3.1\n",
    "- https://huggingface.co/pyannote/segmentation-3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub import notebook_login\n",
    "\n",
    "!pip install -r speaker_diarization.txt\n",
    "!pip install -U \"huggingface_hub[cli]\"\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines.whisper_diarize import ASRDiarizationPipeline\n",
    "from whisperplus import download_youtube_to_mp3, format_speech_to_dialogue\n",
    "\n",
    "audio_path = download_youtube_to_mp3(\"https://www.youtube.com/watch?v=mRB14sFHw2E\")\n",
    "\n",
    "device = \"cuda\"  # cpu or mps\n",
    "pipeline = ASRDiarizationPipeline.from_pretrained(\n",
    "    asr_model=\"openai/whisper-large-v3\",\n",
    "    diarizer_model=\"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=False,\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "output_text = pipeline(audio_path, num_speakers=2, min_speaker=1, max_speaker=2)\n",
    "dialogue = format_speech_to_dialogue(output_text)\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56941291",
   "metadata": {},
   "source": [
    "### ⭐ RAG - Chat with Video (LanceDB)\n",
    "\n",
    "This part covers the 'Chat with Video' feature using LanceDB. It demonstrates how to interact with a video transcript using a chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers ctransformers langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c108aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines.chatbot import ChatWithVideo\n",
    "\n",
    "chat = ChatWithVideo(\n",
    "    input_file=\"trascript.txt\",\n",
    "    llm_model_name=\"TheBloke/Mistral-7B-v0.1-GGUF\",\n",
    "    llm_model_file=\"mistral-7b-v0.1.Q4_K_M.gguf\",\n",
    "    llm_model_type=\"mistral\",\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")\n",
    "\n",
    "query = \"what is this video about ?\"\n",
    "response = chat.run_query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c4696",
   "metadata": {},
   "source": [
    "### 🌠 RAG - Chat with Video (AutoLLM)\n",
    "\n",
    "This section demonstrates the 'Chat with Video' feature using AutoLLM. It enables querying a video's content through a chat interface, utilizing advanced language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autollm>=0.1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines.autollm_chatbot import AutoLLMChatWithVideo\n",
    "\n",
    "# service_context_params\n",
    "system_prompt = \"\"\"\n",
    "You are an friendly ai assistant that help users find the most relevant and accurate answers\n",
    "to their questions based on the documents you have access to.\n",
    "When answering the questions, mostly rely on the info in documents.\n",
    "\"\"\"\n",
    "query_wrapper_prompt = \"\"\"\n",
    "The document information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Using the document information and mostly relying on it,\n",
    "answer the query.\n",
    "Query: {query_str}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "chat = AutoLLMChatWithVideo(\n",
    "    input_file=\"input_dir\",  # path of mp3 file\n",
    "    openai_key=\"YOUR_OPENAI_KEY\",  # optional\n",
    "    huggingface_key=\"YOUR_HUGGINGFACE_KEY\",  # optional\n",
    "    llm_model=\"gpt-3.5-turbo\",\n",
    "    llm_max_tokens=\"256\",\n",
    "    llm_temperature=\"0.1\",\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    embed_model=\"huggingface/BAAI/bge-large-zh\",  # \"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "query = \"what is this video about ?\"\n",
    "response = chat.run_query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ed48e",
   "metadata": {},
   "source": [
    "### 🎙️ Text to Speech\n",
    "\n",
    "Finally, this section covers converting text to speech using WhisperPlus, demonstrating how to generate spoken audio from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50801820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperplus.pipelines.text2speech import TextToSpeechPipeline\n",
    "\n",
    "tts = TextToSpeechPipeline(model_id=\"suno/bark\")\n",
    "audio = tts(text=\"Hello World\", voice_preset=\"v2/en_speaker_6\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
